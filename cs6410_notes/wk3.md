# Scanners

3 different types of analysis required:
1. Lexical analysis
	- Scanner
2. Syntax analysis
	- Scanner (checking for incorrect syntax) and parser
3. Semantic analysis
	- Parser 
- Usually lexical and syntax analysis are done in one go 

There is a nice relationship between regular expressions and automata
- Not always performant when implemented

Scanner reads 1 character at a time to turn source code into <b>token stream</b>
- Keywords, operators, variables, constants (also strips out white space, comments)

<b>Alphabet</b>: finite set of symbols and characters
<b>String</b>: finite, possibly empty sequence of symbols from an alphabet
<b>Language</b>: a set of strings (possibly empty or infinite if you don't restrict the size of your strings)

Finite specifications of languages:
- <b>Automaton</b>: a recognizer - a machine that accepts all strings in a language (and rejects all other strings)
	- A recognizer takes a string and determines whether it is part of a specific language
- <b>Grammar</b>: a generator - a system for producing all strings in the language (and no other strings)
	- Can generate syntactically valid strings but not necessarily semantically valid

A particular language may be specified by many different grammars and automata 
- A grammar or automaton specifies only one language

<b>Backus-Naur Form (BNF</b>: a syntax for describing grammars in terms of transformation rules

## Regular Expressions and Finite Automata (FA)

Lexical grammar (structure) of most programming languages can be specified with <b>regular expressions</b> (sometimes a little cheating is needed)
- RE can be transformed into <b>Non-deterministic finite automata (NDFA/NFA)</b>

Tokens can be recognized by a <b>deterministic finite automaton (DFA)</b>
- Can be either table-driven or built by hand, based on lexical grammar

How to go from NFA into DFA?

Regular expressions are defined over some alphabet $\Sigma$
- For programming languages, alphabet is usually ASCII or Unicode

If $re$ is a regular expression, $L(re)$ is the <b>language</b> (set of strings) generated by $re$

<b>Finite automata</b> can be used to recognize strings generated by regular expressions

A <b>finite state automaton</b> is a formal mathematical object, defined as a five-tuple $(S, \Sigma, \delta, s_0, S_A)$:
- $S$ - the finite set of states of an automaton
- $\Sigma$ - finite set of transition states
- $\delta(s, c)$ - transition function that, for each state $s$, and each character (symbol) from the set $\{ \Sigma \cup \epsilon \}$ gives a set of new states 
- $s_0$ - initial (start) state
- $S_A$ set of accepting (final) states

FSA can be classified into three major groups:
1. **Acceptors (recognizers)**: compute Boolean functions.
	- Accept or reject the inputs given to them
2. **Classifiers**: has more than two final states and it gives a single output when it terminates
3. **Transducers**: produces outputs based on current input and/or previous state
	- Can be two types:
		1. Mealy machine
		2. Moore machine 

We want DFA for speed (no backtracking)
- But conversion from RE to NFA is easy
- There are well-defined procedures for converting a NFA to an equivalent DFA:
	- Subset construction
	- Brzozowski's algorithm

## Thompson's Construction

From RE to NFA

**Base cases**
- The construction begins by building trivial NFAs for each character in the alphabet (including $\epsilon$-transistion)
- Each NFA has one start and one accepting state









